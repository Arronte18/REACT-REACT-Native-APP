{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1eda60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id   latitude   longitude  stars  review_count  \\\n",
      "0  Pns2l4eNsfO8kk83dixA6A  34.426679 -119.711197    5.0             7   \n",
      "1  mpf3x-BjTdTEA3yCZrAYPw  38.551126  -90.335695    3.0            15   \n",
      "2  tUFrWirKiKi_TAnsVWINQQ  32.223236 -110.880452    3.5            22   \n",
      "3  MTSW4McQd7CbVtyjqoe9mw  39.955505  -75.155564    4.0            80   \n",
      "4  mWMc6_wTdE0EUBKIGXDVfA  40.338183  -75.471659    4.5            13   \n",
      "\n",
      "                                          categories  \n",
      "0  Doctors, Traditional Chinese Medicine, Naturop...  \n",
      "1  Shipping Centers, Local Services, Notaries, Ma...  \n",
      "2  Department Stores, Shopping, Fashion, Home & G...  \n",
      "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...  \n",
      "4                          Brewpubs, Breweries, Food  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from the JSON file\n",
    "with open('yelp_academic_dataset_business.json', 'r', encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# create a DataFrame from the data\n",
    "business_df = pd.DataFrame(data)\n",
    "\n",
    "# optionally, you can drop any columns that you don't need\n",
    "business_df = business_df.drop(columns=['hours', 'attributes', 'address', 'city', 'state', 'postal_code', 'is_open', 'name'])\n",
    "\n",
    "# print the first few rows of the DataFrame\n",
    "print(business_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2131ba8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m business_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m business_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the categories column into multiple columns using one-hot encoding\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[43mbusiness_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Combine the original dataframe with the new one-hot encoded columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m business_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([business_df, categories], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/strings/accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/strings/accessor.py:2187\u001b[0m, in \u001b[0;36mStringMethods.get_dummies\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;124;03mReturn DataFrame of dummy/indicator variables for Series.\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;124;03m2  1  0  1\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;66;03m# we need to cast to Series of strings as only that has all\u001b[39;00m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;66;03m# methods available for making the dummies...\u001b[39;00m\n\u001b[0;32m-> 2187\u001b[0m result, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_get_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(\n\u001b[1;32m   2189\u001b[0m     result,\n\u001b[1;32m   2190\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   2191\u001b[0m     expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2192\u001b[0m     returns_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2193\u001b[0m )\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/strings/object_array.py:375\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_get_dummies\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tags2):\n\u001b[1;32m    374\u001b[0m     pat \u001b[38;5;241m=\u001b[39m sep \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m+\u001b[39m sep\n\u001b[0;32m--> 375\u001b[0m     dummies[:, i] \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dummies, tags2\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/strings/object_array.py:375\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_get_dummies.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tags2):\n\u001b[1;32m    374\u001b[0m     pat \u001b[38;5;241m=\u001b[39m sep \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m+\u001b[39m sep\n\u001b[0;32m--> 375\u001b[0m     dummies[:, i] \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(arr\u001b[38;5;241m.\u001b[39mto_numpy(), \u001b[38;5;28;01mlambda\u001b[39;00m x: pat \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dummies, tags2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trim spaces from the category names\n",
    "business_df['categories'] = business_df['categories'].str.strip()\n",
    "\n",
    "# Split the categories column into multiple columns using one-hot encoding\n",
    "categories = business_df['categories'].str.get_dummies(sep=',')\n",
    "\n",
    "# Combine the original dataframe with the new one-hot encoded columns\n",
    "business_df = pd.concat([business_df, categories], axis=1)\n",
    "\n",
    "# Drop the original categories column\n",
    "business_df.drop('categories', axis=1, inplace=True)\n",
    "\n",
    "print(business_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f1c51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id      object\n",
      "latitude        float64\n",
      "longitude       float64\n",
      "stars           float64\n",
      "review_count      int64\n",
      "                 ...   \n",
      "Wraps             int64\n",
      "Yelp Events       int64\n",
      "Yoga              int64\n",
      "Ziplining         int64\n",
      "Zoos              int64\n",
      "Length: 2459, dtype: object\n",
      "review_count            6745508\n",
      " Restaurants              36978\n",
      " Food                     20998\n",
      " Shopping                 18915\n",
      "Restaurants               15290\n",
      "                         ...   \n",
      "Lahmacun                      1\n",
      "Land Surveying                1\n",
      " Concept Shops                1\n",
      "Gemstones & Minerals          1\n",
      "Skiing                        1\n",
      "Length: 2455, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(business_df.dtypes)\n",
    "# Identify the columns that were one-hot encoded\n",
    "one_hot_cols = [col for col in business_df.columns if business_df[col].dtype == 'int64']\n",
    "\n",
    "# Sum the one-hot encoded columns to get the total count for each category\n",
    "category_counts = business_df[one_hot_cols].sum()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "category_counts = category_counts.sort_values(ascending=False)\n",
    "\n",
    "# Print the category counts\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6978f5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user_id             business_id  stars\n",
      "0  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw    3.0\n",
      "1  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ    5.0\n",
      "2  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A    3.0\n",
      "3  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA    5.0\n",
      "4  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ    4.0\n"
     ]
    }
   ],
   "source": [
    "# load the data from the JSON file\n",
    "with open('yelp_academic_dataset_review.json', 'r', encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# create a DataFrame from the data\n",
    "review_df = pd.DataFrame(data)\n",
    "\n",
    "# drop columns\n",
    "review_df = review_df.drop(columns=['review_id', 'useful', 'funny', 'cool', 'text', 'date'])\n",
    "\n",
    "# print the first few rows of the DataFrame\n",
    "print(review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96f76d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(business_df, review_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a pivot table of the merged dataframe to create the user-item matrix\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m user_item_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstars\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbusiness_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Split the user-item matrix into training and test sets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(user_item_matrix, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:97\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     94\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TheFastSupper/data/supper/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:143\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(i)\n\u001b[1;32m    145\u001b[0m to_filter \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;241m+\u001b[39m values:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stars'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Merge the dataframes on the 'business_id' column\n",
    "merged_df = pd.merge(business_df, review_df, on='business_id', how='inner')\n",
    "\n",
    "# Create a pivot table of the merged dataframe to create the user-item matrix\n",
    "user_item_matrix = pd.pivot_table(merged_df, values='stars', index='user_id', columns='business_id')\n",
    "\n",
    "# Split the user-item matrix into training and test sets\n",
    "train_data, test_data = train_test_split(user_item_matrix, test_size=0.2)\n",
    "\n",
    "# Calculate the cosine similarity between users\n",
    "user_similarity = cosine_similarity(train_data.fillna(0))\n",
    "\n",
    "# Define the number of recommended businesses\n",
    "n = 10\n",
    "\n",
    "# Calculate the cosine similarity between businesses\n",
    "business_similarity = cosine_similarity(train_data.T.fillna(0))\n",
    "\n",
    "# Create a dictionary to store the recommended businesses for each business_id\n",
    "recommended_businesses = {}\n",
    "\n",
    "# Iterate through each business_id and find the top n similar businesses based on user ratings\n",
    "for business_id in train_data.columns:\n",
    "    similar_businesses = business_similarity[business_id].argsort()[-(n+1):-1]\n",
    "    recommended_businesses[business_id] = list(train_data.columns[similar_businesses])\n",
    "\n",
    "# Define a function to calculate the mean average precision at k for a user\n",
    "def mapk(actual, predicted, k):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return score / min(len(actual), k)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "k = 10 # number of recommended businesses\n",
    "mapk_scores = []\n",
    "\n",
    "for user_id in test_data.index:\n",
    "    actual_businesses = list(test_data.loc[user_id].dropna().index)\n",
    "    if len(actual_businesses) >= k:\n",
    "        recommended_businesses = []\n",
    "        for business_id in train_data.columns:\n",
    "            if business_id not in actual_businesses:\n",
    "                predicted_rating = np.dot(user_similarity[np.where(train_data.index == user_id)[0][0]],\n",
    "                                          train_data[business_id].fillna(0)) / np.sum(user_similarity[np.where(train_data.index == user_id)[0][0]])\n",
    "                recommended_businesses.append((business_id, predicted_rating))\n",
    "        recommended_businesses.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommended_businesses = [x[0] for x in recommended_businesses[:k]]\n",
    "        score = mapk(actual_businesses, recommended_businesses, k)\n",
    "        mapk_scores.append(score)\n",
    "\n",
    "# Calculate the mean average precision at k for the test set\n",
    "mean_apk = np.mean(mapk_scores)\n",
    "print(f'Mean average precision at k = {k} on the test set: {mean_apk:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
